{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eccf21c-b74c-4f39-acf9-f62605a4dba9",
   "metadata": {},
   "source": [
    "# 实验六 决策树分类器\n",
    "## 2210529 罗瑞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee9769-fbbe-4f53-894b-1c42361a0d99",
   "metadata": {},
   "source": [
    "## 基本要求\n",
    "\n",
    "基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "## 中级要求\n",
    "\n",
    "对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "## 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458d7df-8162-499d-9e0a-98f35bfa7bbf",
   "metadata": {},
   "source": [
    "## ***基本要求实现***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3062c-5fc6-4801-8e17-d5458c59cf21",
   "metadata": {},
   "source": [
    "### **加载数据**\n",
    "使用load_txt方法从train1.scv 中读入数据，将离散的特征值（如“青绿”“蜷缩”等）转换为索引表示，将会返回一个NumPy数组，数组的每一行表示一个样本。\n",
    "输出格式每一行的格式是 [编号, 色泽索引, 根蒂索引, 敲声索引, 纹理索引, 标签索引]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "019c8df4-0c9f-4b80-9ab6-eb8d1c74af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "feature_dict = {\"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "                \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "                \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "                \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]\n",
    "                }\n",
    "lable_list = [\"否\", \"是\"]\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\"]\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()  # 读取表头，跳过\n",
    "        line = f.readline()  # 读取数据第一行\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')  # 按逗号分割每行数据\n",
    "            re = []\n",
    "            re.append(int(d[0]))  # 第0列是编号，作为辅助信息\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]))  # 将“色泽”属性映射为索引\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]))  # 将“根蒂”属性映射为索引\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]))  # 将“敲声”属性映射为索引\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]))  # 将“纹理”属性映射为索引\n",
    "            re.append(lable_list.index(d[-1]))  # 最后一列是标签（“是”或“否”），映射为索引\n",
    "            ans.append(np.array(re))  # 存储为NumPy数组\n",
    "            line = f.readline()  # 继续读取下一行\n",
    "    return np.array(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde1089-141c-45dc-8a83-73738b09be5d",
   "metadata": {},
   "source": [
    "### **定义信息熵与信息增益的计算**\n",
    "\n",
    "分别使用ent和gain函数。\n",
    "\n",
    "#### 信息熵的计算:ent函数\n",
    "$$H(D)=-\\sum_{k=1}^Kp_k\\log_2p_k$$\n",
    "输入：类别标签数组D（如[0, 1, 1, 0]）。\n",
    "输出：数据集的熵，表示类别分布的不确定性。\n",
    "\n",
    "\n",
    "#### 信息增益的计算gain函数\n",
    "$$Gain(D,A)=H(D)-\\sum_{v\\in A}\\frac{|D_v|}{|D|}H(D_v)$$\n",
    "输入：样本集X，类别标签Y，属性索引attr。\n",
    "输出：在该属性上的信息增益。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317d748d-98a9-445d-bb7a-16c935c01563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent(D):\n",
    "    s = 0\n",
    "    for k in set(D):\n",
    "        p_k = np.sum(np.where(D == k, 1, 0)) / np.shape(D)[0]  # 每个类别k的概率\n",
    "        if p_k == 0:\n",
    "            continue  # 概率为0时，熵贡献为0\n",
    "        s += p_k * np.log2(p_k)  # 计算熵公式\n",
    "    return -s\n",
    "\n",
    "def gain(X, Y, attr):\n",
    "    x_attr_col = X[:, attr]  # 获取当前属性列\n",
    "    ent_Dv = []  # 子集的熵\n",
    "    weight_Dv = []  # 子集的权重\n",
    "    for x_v in set(x_attr_col):  # 遍历属性的所有取值\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v)\n",
    "        y_x_equal_v = Y[index_x_equal_v]  # 对应的子集标签\n",
    "        ent_Dv.append(ent(y_x_equal_v))  # 子集的熵\n",
    "        weight_Dv.append(np.shape(y_x_equal_v)[0] / np.shape(Y)[0])  # 子集权重\n",
    "    return ent(Y) - np.sum(np.array(ent_Dv) * np.array(weight_Dv))  # 信息增益公式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a2a61-b28c-4a9c-a239-092fdbcdcf20",
   "metadata": {},
   "source": [
    "### **定义决策树节点：Node**\n",
    "\n",
    "如果attr == -1，表示这是叶节点，label存储类别。\n",
    "如果label == π，表示非叶节点，attr存储划分属性。\n",
    "children存储当前节点的所有子节点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff941116-3866-46c8-bdd0-59646cc08fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attr, label, v):\n",
    "        self.attr = attr  # 节点划分的属性索引（-1 表示叶节点）\n",
    "        self.label = label  # 节点的类别标签（π 表示非叶节点）\n",
    "        self.attr_v = v  # 父节点划分属性的值（仅对子节点有意义）\n",
    "        self.children = []  # 子节点列表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fa134-9cdd-400e-8ea9-7c6b7f833886",
   "metadata": {},
   "source": [
    "### **定义函数验证属性上的值是否相同** \n",
    "is_same_on_attr 通常用于决策树构建的特殊情况处理：\n",
    "\n",
    "检查样本是否无法划分：\n",
    "\n",
    "在决策树递归过程中，如果样本的所有属性值在指定的属性集合上完全相同，则说明无法通过这些属性进一步划分。\n",
    "终止递归，生成叶节点：\n",
    "\n",
    "当所有样本的属性值完全一致时，决策树会选择直接生成叶节点，而不再继续分裂。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcf0c60-d709-4d9d-b5bb-9f31425cbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_on_attr(X, attrs):#验证属性上的值是否均相同\n",
    "    X_a = X[:, attrs]\n",
    "    target = X_a[0]\n",
    "    for r in range(X_a.shape[0]):\n",
    "        row = X_a[r]\n",
    "        if (row != target).any():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081da260-f075-4945-8759-c9c424bc89d5",
   "metadata": {},
   "source": [
    "### **构建决策树**（函数dicision_tree_init）\n",
    "递归地构造决策树：\n",
    "\n",
    "检查是否可以直接生成叶节点。\n",
    "\n",
    "选择信息增益最大的属性作为划分依据。\n",
    "\n",
    "对于每个取值生成子节点，并递归调用自身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64ca86b-ae85-43b8-bc2b-f7326ea3d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicision_tree_init(X, Y, attrs, root, purity_cal):\n",
    "    # 递归基\n",
    "    if len(set(Y)) == 1:\n",
    "        root.attr = np.pi\n",
    "        root.label = Y[0]\n",
    "        return None\n",
    "\n",
    "    if len(attrs) == 0 or is_same_on_attr(X, attrs):\n",
    "        root.attr = np.pi\n",
    "        # Y 中出现次数最多的label设定为node的label\n",
    "        root.label = np.argmax(np.bincount(Y))\n",
    "        return None\n",
    "\n",
    "    # 计算每个attr的划分收益\n",
    "    purity_attrs = []\n",
    "    for i, a in enumerate(attrs):\n",
    "        p = purity_cal(X, Y, a)\n",
    "        purity_attrs.append(p)\n",
    "    #print(purity_attrs)\n",
    "    chosen_index = purity_attrs.index(max(purity_attrs))\n",
    "    chosen_attr = attrs[chosen_index]\n",
    "\n",
    "    root.attr = chosen_attr\n",
    "    root.label = np.pi\n",
    "\n",
    "    del attrs[chosen_index]\n",
    "\n",
    "    x_attr_col = X[:, chosen_attr]\n",
    "    # 离散数据处理\n",
    "    for x_v in set(X[:, chosen_attr]):\n",
    "        n = Node(-1, -1, x_v)\n",
    "        root.children.append(n)\n",
    "        # 不可能Dv empty 要是empty压根不会在set里\n",
    "        # 选出 X[attr] == x_v的行\n",
    "\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v)\n",
    "        X_x_equal_v = X[index_x_equal_v]\n",
    "        Y_x_equal_v = Y[index_x_equal_v]\n",
    "        dicision_tree_init(X_x_equal_v, Y_x_equal_v, attrs, n, purity_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d80564-884d-438f-af7d-ca029d6e3179",
   "metadata": {},
   "source": [
    "### **基于构建的决策树进行预测** 决策树预测：dicision_tree_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27385a2b-5687-4cf7-a818-c6b22391d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicision_tree_predict(x, tree_root):\n",
    "    if tree_root.label != np.pi:  # 如果是叶节点，返回类别\n",
    "        return tree_root.label\n",
    "    for child in tree_root.children:  # 遍历子节点，找到匹配的分支\n",
    "        if child.attr_v == x[tree_root.attr]:\n",
    "            return dicision_tree_predict(x, child)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec302df-3a5b-4567-b7db-9b99e343407a",
   "metadata": {},
   "source": [
    "最后在主函数中进行决策树的构建和基于其进行预测，最后再计算分类准确度：\n",
    "具体过程是；\n",
    "\n",
    "加载训练集与测试集。\n",
    "\n",
    "构建ID3决策树并存储在根节点r。\n",
    "\n",
    "使用测试集进行预测，并计算分类精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4662cf55-0d41-4a81-8f61-fdf641b1e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ans = load_txt(\"Watermelon-train1.csv\")\n",
    "    X_train = ans[:, 1: -1]\n",
    "    Y_train = ans[:, -1].astype(np.int64)\n",
    "    test_data = load_txt(\"Watermelon-test1.csv\")\n",
    "    X_test = test_data[:, 1:-1]\n",
    "    Y_test = test_data[:, -1].astype(np.int64)\n",
    "    r = Node(-1, -1, -1)\n",
    "    attrs = [0, 1, 2, 3]\n",
    "    dicision_tree_init(X_train, Y_train, attrs, r, gain)\n",
    "    y_predict = [dicision_tree_predict(x, r) for x in X_test]\n",
    "    accuracy = sum(1 for y_true, y_pred in zip(Y_test, y_predict) if y_true == y_pred) / len(Y_test)\n",
    "    print('accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75675b67-7242-4f35-8e33-d701b52242cc",
   "metadata": {},
   "source": [
    "分类精度为0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e35e8c-249a-47e5-acca-dda378210781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "feature_dict = {\"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "                \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "                \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "                \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]\n",
    "                }\n",
    "lable_list = [\"否\", \"是\"]\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\"]\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()  # 读取表头，跳过\n",
    "        line = f.readline()  # 读取数据第一行\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')  # 按逗号分割每行数据\n",
    "            re = []\n",
    "            re.append(int(d[0]))  # 第0列是编号，作为辅助信息\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]))  # 将“色泽”属性映射为索引\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]))  # 将“根蒂”属性映射为索引\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]))  # 将“敲声”属性映射为索引\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]))  # 将“纹理”属性映射为索引\n",
    "            re.append(lable_list.index(d[-1]))  # 最后一列是标签（“是”或“否”），映射为索引\n",
    "            ans.append(np.array(re))  # 存储为NumPy数组\n",
    "            line = f.readline()  # 继续读取下一行\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9bf63-0ce5-44a8-94a2-b50890cfb618",
   "metadata": {},
   "source": [
    "## ***中级要求实现（实现CART算法能够处理连续型特征）***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6466b987-e911-4505-8b56-1dfdf0119445",
   "metadata": {},
   "source": [
    "#### 观察数据集2可知，多了连续型的属性：密度，因此实验需要首先再次观察一下数据集属性组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96ed9336-f4bf-4974-ab58-134a8a7275a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.     0.     0.     0.     0.     0.697  1.   ]\n",
      " [ 2.     1.     0.     1.     0.     0.774  1.   ]\n",
      " [ 3.     1.     0.     0.     0.     0.634  1.   ]\n",
      " [ 4.     0.     0.     1.     0.     0.608  1.   ]\n",
      " [ 5.     2.     0.     0.     0.     0.556  1.   ]\n",
      " [ 6.     0.     1.     0.     0.     0.403  1.   ]\n",
      " [ 7.     1.     1.     0.     1.     0.481  1.   ]\n",
      " [ 8.     1.     1.     0.     0.     0.437  1.   ]\n",
      " [ 9.     1.     1.     1.     1.     0.666  0.   ]\n",
      " [10.     0.     2.     2.     0.     0.243  0.   ]\n",
      " [11.     2.     2.     2.     2.     0.245  0.   ]\n",
      " [12.     2.     0.     0.     2.     0.343  0.   ]\n",
      " [13.     0.     1.     0.     1.     0.639  0.   ]\n",
      " [14.     2.     1.     1.     1.     0.657  0.   ]\n",
      " [15.     1.     1.     0.     0.     0.36   0.   ]\n",
      " [16.     2.     0.     0.     2.     0.593  0.   ]\n",
      " [17.     0.     0.     1.     1.     0.719  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "feature_dict = {\n",
    "    \"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "    \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "    \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "    \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]\n",
    "}\n",
    "lable_list = [\"否\", \"是\"]#是否是“好瓜,属性名为好瓜”\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"密度\"]  # 加入新的连续型特征“密度”\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()  # 读取表头，跳过\n",
    "        line = f.readline()  # 读取数据第一行\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')  # 按逗号分割每行数据\n",
    "            re = []\n",
    "            re.append(int(d[0]))  # 第0列是编号，作为辅助信息\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]))  # 将“色泽”属性映射为索引\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]))  # 将“根蒂”属性映射为索引\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]))  # 将“敲声”属性映射为索引\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]))  # 将“纹理”属性映射为索引\n",
    "            re.append(float(d[5]))  # 第5列是连续型变量“密度”，以浮点数形式读取\n",
    "            re.append(lable_list.index(d[-1]))  # 最后一列是标签（“是”或“否”），映射为索引\n",
    "            ans.append(np.array(re))  # 存储为NumPy数组\n",
    "            line = f.readline()  # 继续读取下一行\n",
    "    return np.array(ans, dtype=np.float64)  # 指定返回的数组类型为float64\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = load_txt(\"Watermelon-train2.csv\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa51bd0-3210-412f-81fc-0ee00139b385",
   "metadata": {},
   "source": [
    "### 可以看到多了一个属性列为密度，是一个连续变量，后面我们将利用CART算法使用二元切分的方法来处理连续变量密度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d790ac5-333c-44c7-a81f-2cc7c25e8225",
   "metadata": {},
   "source": [
    "### **数据集的读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cfdcd09-1497-4905-9594-bf1849b37bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train2_dataset=pd.read_csv(\"Watermelon-train2.csv\",encoding='GBK')\n",
    "test2_dataset=pd.read_csv(\"Watermelon-test2.csv\",encoding='GBK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303d3fb-0a0c-4cfb-a460-64840c96bf97",
   "metadata": {},
   "source": [
    "### **基尼指数的计算实现，以及基于二分方法寻找续型特征；密度的最佳分割点**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8318c-ba13-46ab-9ffc-dbfb77b36a99",
   "metadata": {},
   "source": [
    "#### 注意是/否列属性名为“好瓜”，下面将计算CART算法对应的基尼系数,对于Ck：（𝐷中属于第𝑘类的样本子集，大K是类的个数）公式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011496-d5b1-4c3e-a3da-7e5cef028814",
   "metadata": {},
   "source": [
    "$$Gini(D)=1-\\sum_{k=1}^K\\left(\\frac{|C_k|}{|D|}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75174eb0-bd56-4292-9057-57a75f7bd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算Gini 指数\n",
    "def caculate_GINI(data):\n",
    "    # 计算目标列中每个类别的概率\n",
    "    class_probs = data['好瓜'].value_counts(normalize=True)\n",
    "    # 计算 Gini 指数\n",
    "    gini_index = 1 - sum([p ** 2 for p in class_probs])\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a951b14-e030-4b97-aa08-c94a5fa575b4",
   "metadata": {},
   "source": [
    "compute_gini_for_split 函数是计算特定特征对于数据集分割后的加权 Gini 指数的函数。它通过遍历特征的所有可能取值，将数据集分割为子集并计算加权 Gini，从而帮助决策树算法选择最优的分割特征。这里使用可以利用 groupby 操作，同时计算加权 Gini 的过程也可以简化。我们可以直接对数据进行分组，并计算每个组的 Gini 值，最后根据每个组的比例计算加权 Gini。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a9a97ed-d179-446d-847a-247dc2bc7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算特定特征的 Gini 分裂指数\n",
    "def calculate_GINI_for_split(data, feature):\n",
    "    # 使用groupby分组，并计算每个组的Gini\n",
    "    weighted_gini = data.groupby(feature).apply(lambda subset: compute_gini_index(subset)).mul(\n",
    "        data.groupby(feature).size() / len(data)\n",
    "    ).sum()\n",
    "\n",
    "    return weighted_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12215b-8cc7-47fc-88d2-d9c211e51d3a",
   "metadata": {},
   "source": [
    "对于每个特征：\n",
    "\n",
    "按照特征值的大小进行排序。\n",
    "\n",
    "计算相邻两个值之间的中点，作为可能的分割点。\n",
    "\n",
    "对于每个分割点：\n",
    "\n",
    "将数据集根据分割点划分为左子集和右子集。\n",
    "\n",
    "计算左子集和右子集的 Gini 指数。\n",
    "\n",
    "计算加权 Gini 指数。\n",
    "\n",
    "选择加权 Gini 指数最小的分割点，作为最佳分割点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f36c13-0df2-4540-a934-ac1fecee6dc1",
   "metadata": {},
   "source": [
    "在找到可能的分割点之后，我们将数据集分成两个子集 (左子集和右子集)。每个子集的 Gini 指数可\n",
    "以单独计算，然后加权平均，以得到整个分割点的加权 Gini 指数。\n",
    "假设在某个分割点，数据集$D$被划分为两个子集$D_\\mathrm{leff}$和$D_\\mathrm{right}$,它们的大小分别为$n_\\mathrm{left}$和$n_\\mathrm{right}$\n",
    ",且总数据集大小为$n$,则加权 Gini 指数计算公式为：\n",
    "$$Gini_{{\\mathrm{split}}}=\\frac{n_{{\\mathrm{left}}}}{n}\\cdot Gini(D_{{\\mathrm{left}}})+\\frac{n_{{\\mathrm{right}}}}{n}\\cdot Gini(D_{{\\mathrm{right}}})$$\n",
    "\n",
    "即：\n",
    "\n",
    "1. 计算左子集的 Gini 指数$Gini(D_\\mathrm{left})$和右子集的 Gini 指数$Gini(D_\\mathrm{right})$。\n",
    "2. 根据左子集和右子集的样本数量，计算加权平均的 Gini 指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "753ab44d-4388-40f0-ad2a-fab7692d7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 Gini 指数，找到连续型特征的最佳分割点，\n",
    "def find_best_split_point(data, feature):\n",
    "    # 获取特征的唯一值并排序\n",
    "    sorted_values = sorted(data[feature].unique())\n",
    "\n",
    "    # 计算最佳分割点和最低 Gini\n",
    "    def calculate_weighted_gini(split_point):\n",
    "        left_subset = data[data[feature] <= split_point]\n",
    "        right_subset = data[data[feature] > split_point]\n",
    "\n",
    "        gini_left = caculate_GINI(left_subset)\n",
    "        gini_right = caculate_GINI(right_subset)\n",
    "\n",
    "        weighted_gini = (len(left_subset) / len(data)) * gini_left + (len(right_subset) / len(data)) * gini_right\n",
    "        return weighted_gini\n",
    "\n",
    "    # 使用min函数简化计算和比较\n",
    "    best_split_point, lowest_gini_index = min(\n",
    "        (( (sorted_values[i] + sorted_values[i + 1]) / 2, calculate_weighted_gini((sorted_values[i] + sorted_values[i + 1]) / 2) )\n",
    "         for i in range(len(sorted_values) - 1)),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "\n",
    "    return best_split_point, lowest_gini_index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758999c7-5df2-4848-abd8-8243b8151fd0",
   "metadata": {},
   "source": [
    "### **CART决策树的构建**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57750700-f06e-4eea-a8ee-aebf2a1fbc1a",
   "metadata": {},
   "source": [
    "**选择最佳分割点**：通过计算所有特征的 Gini 值来选择最佳的分割特征。如果是连续特征，则找到最佳的分割点；如果是离散特征，则直接计算 Gini 指数。\n",
    "\n",
    "**进行递归地构建子树**：递归地对左右子集或离散特征的子集进行相同的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92bbecfd-b008-4bfc-b593-0b057fe3040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CART_tree(data, features):\n",
    "    target = '好瓜'\n",
    "\n",
    "    # 如果所有目标值相同，返回该目标值（叶节点）\n",
    "    if data[target].nunique() == 1:\n",
    "        return data[target].iloc[0]\n",
    "\n",
    "    # 如果没有剩余的特征用于分裂，返回目标列中最常见的类别（众数）\n",
    "    if not features:\n",
    "        return data[target].mode()[0]\n",
    "\n",
    "    # 初始化最佳属性和 Gini 指数\n",
    "    best_feature, best_split_point, lowest_gini, is_continuous = None, None, float(\"inf\"), False\n",
    "\n",
    "    for feature in features:\n",
    "        if feature == '密度':\n",
    "            # 连续特征寻找最佳分割点\n",
    "            split_point, gini_value = find_best_split_point(data, feature)\n",
    "            if gini_value < lowest_gini:\n",
    "                best_feature, best_split_point, lowest_gini, is_continuous = feature, split_point, gini_value, True\n",
    "        else:\n",
    "            # 离散特征直接计算 Gini 指数\n",
    "            gini_value = caculate_GINI_for_split(data, feature)\n",
    "            if gini_value < lowest_gini:\n",
    "                best_feature, lowest_gini, is_continuous = feature, gini_value, False\n",
    "\n",
    "    # 构建树的根节点\n",
    "    cart_tree = {best_feature: {}}\n",
    "\n",
    "    if is_continuous:\n",
    "        # 处理连续特征的分割\n",
    "        left_subset, right_subset = data[data[best_feature] <= best_split_point], data[data[best_feature] > best_split_point]\n",
    "        cart_tree[best_feature][f'<= {best_split_point}'] = build_CART_tree(left_subset, features)\n",
    "        cart_tree[best_feature][f'> {best_split_point}'] = build_CART_tree(right_subset, features)\n",
    "    else:\n",
    "        # 处理离散特征的分割\n",
    "        remaining_features = [f for f in features if f != best_feature]\n",
    "        for value in data[best_feature].unique():\n",
    "            subset = data[data[best_feature] == value]\n",
    "            cart_tree[best_feature][value] = build_CART_tree(subset, remaining_features) if not subset.empty else data[target].mode()[0]\n",
    "\n",
    "    return cart_tree\n",
    "\n",
    "# 过滤掉目标列和 '编号' 列，获取所有特征\n",
    "features_cart = [col for col in train2_dataset.columns if col not in ['好瓜', '编号']]\n",
    "\n",
    "\n",
    "# 构建 CART 决策树\n",
    "my_CART_tree = build_cart_tree(train2_dataset, features_cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2ff7a-84c2-452c-8715-8d09e5603695",
   "metadata": {},
   "source": [
    "#### 这时是我们构建出来的CART决策树应该已经合理地处理密度这个连续特征了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6927ca1-4412-4212-abed-5aaf0064aa0d",
   "metadata": {},
   "source": [
    "### **使用CART决策树对测试集dataset2进行预测并计算准确度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e95ea98-dae3-4d8b-965e-3994bd9fc980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART决策树方法的准确度: 0.8\n"
     ]
    }
   ],
   "source": [
    "def calculate_cart_accuracy_and_predict(tree, test_data, target_column='好瓜'):\n",
    "    correct_predictions = 0\n",
    "    no_branch_predictions = 0  # 记录预测为 None 的样本数\n",
    "\n",
    "    # 遍历测试数据的每一行，进行预测\n",
    "    for _, row in test_data.iterrows():\n",
    "        # 预测一个样本\n",
    "        def predict_cart(tree, sample):\n",
    "            # 如果当前节点是叶节点，直接返回预测值\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "\n",
    "            # 遍历当前节点的特征及其子树\n",
    "            for feature, branches in tree.items():\n",
    "                if feature in sample:\n",
    "                    feature_value = sample[feature]\n",
    "\n",
    "                    # 如果是连续特征的分割，处理 '<=' 和 '>'\n",
    "                    if isinstance(branches, dict):\n",
    "                        for branch_condition in branches.keys():\n",
    "                            # 处理连续特征的分支\n",
    "                            if branch_condition.startswith('<='): \n",
    "                                threshold = float(branch_condition[3:])  # 阈值提取\n",
    "                                if feature_value <= threshold:\n",
    "                                    return predict_cart(branches[branch_condition], sample)\n",
    "                            elif branch_condition.startswith('>'):\n",
    "                                threshold = float(branch_condition[3:])  \n",
    "                                if feature_value > threshold:\n",
    "                                    return predict_cart(branches[branch_condition], sample)\n",
    "\n",
    "                    # 如果是离散特征的分支，直接递归访问子树\n",
    "                    if feature_value in branches:\n",
    "                        return predict_cart(branches[feature_value], sample)\n",
    "\n",
    "            # 如果没有找到合适的分支，返回 None（无法预测）\n",
    "            return None\n",
    "\n",
    "        # 进行预测\n",
    "        predicted_label = predict_cart(tree, row)\n",
    "        actual_label = row[target_column]\n",
    "\n",
    "        # 统计预测正确的样本\n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions += 1\n",
    "        elif predicted_label is None:\n",
    "            no_branch_predictions += 1  # 记录没有找到合适分支的情况\n",
    "\n",
    "    # 计算并返回准确率\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "# 计算并打印准确率\n",
    "cart_accuracy = calculate_cart_accuracy_and_predict(my_CART_tree, test2_dataset)\n",
    "print(f\"CART决策树方法的准确度: {cart_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e836b9-cd8f-4f78-ae56-c9b5ad0976a7",
   "metadata": {},
   "source": [
    "#### 可见其准确度较高，我认为原因主要是CART合理地利用好了连续的密度特征信息，从而做出“高明”的分类决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4b24b-4ced-4b02-b637-0799794ba83b",
   "metadata": {},
   "source": [
    "## ***高级要求实现（使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝）***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28278ea4-650b-45bb-a100-325c4785abdd",
   "metadata": {},
   "source": [
    "**剪枝**：决策树很容易出现过拟合现象。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "解决这个问题的方法称为剪枝，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。\n",
    "\n",
    "决策树的剪枝基本策略有**预剪枝(Pre-Pruning)**和**后剪枝(Post-Pruning)**：\n",
    "\n",
    "预剪枝：是根据⼀些原则极早的停止树增长，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。\n",
    "\n",
    "后剪枝：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后自底向上的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9137f-403e-4eac-a51f-ac3f933a1b93",
   "metadata": {},
   "source": [
    "剪枝的作用：\n",
    "\n",
    "避免过拟合：通过减少树的复杂度，剪枝有助于提高模型的泛化能力。\n",
    "\n",
    "提高模型可解释性：剪枝后的树结构更简洁，减少了冗余的分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb988a3-884e-4e1f-bfda-e1d1303b80c7",
   "metadata": {},
   "source": [
    "**这里两个剪枝均对原剪枝树实现后剪枝算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d8139-73ab-4aa9-8c0a-771f52258706",
   "metadata": {},
   "source": [
    "### **对基本要求实现的决策树的后剪枝**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa295b-7097-4855-b8be-10796135526a",
   "metadata": {},
   "source": [
    "**关键函数设计说明**：\n",
    "\n",
    "post_pruning函数：这是实现后剪枝的主要部分。它递归遍历树的每个节点，并尝试将节点的子树替换为叶节点（即赋予该节点数据集中的多数类标签）。然后，评估剪枝前后的准确度，若剪枝后的准确度不低于原准确度，则保留剪枝后的结构，否则恢复原状。\n",
    "\n",
    "evaluate_accuracy函数：用于计算剪枝后的树在给定数据集上的准确率。它通过遍历测试集，对每个数据点进行预测并与实际标签比较，从而计算准确率。\n",
    "\n",
    "dicision_tree_init和dicision_tree_predict：这两个函数用于构建决策树和预测过程，和我之前的实现都一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d984ec52-5ea8-445e-bf98-afba3135526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after pruning: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "feature_dict = {\"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "                \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "                \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "                \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]\n",
    "                }\n",
    "lable_list = [\"否\", \"是\"]\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\"]\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')\n",
    "            re = []\n",
    "            re.append(int(d[0]))\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]))\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]))\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]))\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]))\n",
    "            re.append(lable_list.index(d[-1]))\n",
    "            ans.append(np.array(re))\n",
    "            line = f.readline()\n",
    "    return np.array(ans)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attr, label, v):\n",
    "        self.attr = attr\n",
    "        self.label = label\n",
    "        self.attr_v = v\n",
    "        self.children = []\n",
    "\n",
    "def is_same_on_attr(X, attrs):\n",
    "    X_a = X[:, attrs]\n",
    "    target = X_a[0]\n",
    "    for r in range(X_a.shape[0]):\n",
    "        row = X_a[r]\n",
    "        if (row != target).any():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def ent(D):\n",
    "    s = 0\n",
    "    for k in set(D):\n",
    "        p_k = np.sum(np.where(D == k, 1, 0)) / np.shape(D)[0]\n",
    "        if p_k == 0:\n",
    "            continue\n",
    "        s += p_k * np.log2(p_k)\n",
    "    return -s\n",
    "\n",
    "def gain(X, Y, attr):\n",
    "    x_attr_col = X[:, attr]\n",
    "    ent_Dv = []\n",
    "    weight_Dv = []\n",
    "    for x_v in set(x_attr_col):\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v)\n",
    "        y_x_equal_v = Y[index_x_equal_v]\n",
    "        ent_Dv.append(ent(y_x_equal_v))\n",
    "        weight_Dv.append(np.shape(y_x_equal_v)[0] / np.shape(Y)[0])\n",
    "    return ent(Y) - np.sum(np.array(ent_Dv) * np.array(weight_Dv))\n",
    "\n",
    "def dicision_tree_init(X, Y, attrs, root, purity_cal):\n",
    "    if len(set(Y)) == 1:\n",
    "        root.attr = np.pi\n",
    "        root.label = Y[0]\n",
    "        return None\n",
    "\n",
    "    if len(attrs) == 0 or is_same_on_attr(X, attrs):\n",
    "        root.attr = np.pi\n",
    "        root.label = np.argmax(np.bincount(Y))\n",
    "        return None\n",
    "\n",
    "    purity_attrs = []\n",
    "    for i, a in enumerate(attrs):\n",
    "        p = purity_cal(X, Y, a)\n",
    "        purity_attrs.append(p)\n",
    "\n",
    "    chosen_index = purity_attrs.index(max(purity_attrs))\n",
    "    chosen_attr = attrs[chosen_index]\n",
    "    root.attr = chosen_attr\n",
    "    root.label = np.pi\n",
    "    del attrs[chosen_index]\n",
    "\n",
    "    x_attr_col = X[:, chosen_attr]\n",
    "    for x_v in set(X[:, chosen_attr]):\n",
    "        n = Node(-1, -1, x_v)\n",
    "        root.children.append(n)\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v)\n",
    "        X_x_equal_v = X[index_x_equal_v]\n",
    "        Y_x_equal_v = Y[index_x_equal_v]\n",
    "        dicision_tree_init(X_x_equal_v, Y_x_equal_v, attrs, n, purity_cal)\n",
    "\n",
    "def dicision_tree_predict(x, tree_root):\n",
    "    if tree_root.label != np.pi:\n",
    "        return tree_root.label\n",
    "\n",
    "    chose_attr = tree_root.attr\n",
    "    for child in tree_root.children:\n",
    "        if child.attr_v == x[chose_attr]:\n",
    "            return dicision_tree_predict(x, child)\n",
    "    return None\n",
    "\n",
    "def post_pruning(X, Y, node):\n",
    "    if node.label != np.pi:\n",
    "        return\n",
    "\n",
    "    for child in node.children:\n",
    "        post_pruning(X, Y, child)\n",
    "\n",
    "    original_accuracy = evaluate_accuracy(X, Y, node)\n",
    "    majority_class = np.argmax(np.bincount(Y))\n",
    "    node.children = []\n",
    "    node.label = majority_class\n",
    "    new_accuracy = evaluate_accuracy(X, Y, node)\n",
    "\n",
    "    if new_accuracy < original_accuracy:\n",
    "        node.label = majority_class\n",
    "        node.children = []\n",
    "\n",
    "def evaluate_accuracy(X, Y, node):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        predictions.append(dicision_tree_predict(x, node))\n",
    "    correct_predictions = sum(1 for y_true, y_pred in zip(Y, predictions) if y_true == y_pred)\n",
    "    total_predictions = len(Y)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ans = load_txt(\"Watermelon-train1.csv\")\n",
    "    X_train = ans[:, 1: -1]\n",
    "    Y_train = ans[:, -1].astype(np.int64)\n",
    "    test_data = load_txt(\"Watermelon-test1.csv\")\n",
    "    X_test = test_data[:, 1:-1]\n",
    "    Y_test = test_data[:, -1].astype(np.int64)\n",
    "    \n",
    "    r = Node(-1, -1, -1)\n",
    "    attrs = [0, 1, 2, 3]\n",
    "\n",
    "    dicision_tree_init(X_train, Y_train, attrs, r, gain)\n",
    "\n",
    "    post_pruning(X_train, Y_train, r)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i]\n",
    "        y_p = dicision_tree_predict(x, r)\n",
    "        y_predict.append(y_p)\n",
    "\n",
    "    correct_predictions = sum(1 for y_true, y_pred in zip(Y_test, y_predict) if y_true == y_pred)\n",
    "    total_predictions = len(Y_test)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print('accuracy after pruning:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453af75-0d8a-4c0a-a5f5-44621de3f221",
   "metadata": {},
   "source": [
    "#### 发现剪枝后的精度为0.5，不如之前的不剪枝的决策树，精度出现了下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84952f5c-9c14-4479-b0b9-570041cc6188",
   "metadata": {},
   "source": [
    "### **对中级要求实现的CART决策树的后剪枝**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fac7a-14f4-476e-a434-ea585a5041dc",
   "metadata": {},
   "source": [
    "#### 主要函数设计说明：\n",
    "\n",
    "prune_tree: 这是后剪枝的主函数，它递归遍历每个子树，如果在某个节点剪枝后的误差与剪枝前的误差差距较小（小于一个阈值 alpha），就将该节点剪枝为叶节点。\n",
    "\n",
    "calculate_tree_error: 计算树在验证集上的误差（即预测不准确的比例）。\n",
    "\n",
    "calculate_tree_error_after_pruning: 计算剪枝后树的误差。剪枝后即用叶节点替代树的所有子树。\n",
    "\n",
    "get_leaf_value: 用于获取树的叶节点值，即树所有分支的最终预测值。\n",
    "\n",
    "alpha: 控制剪枝的程度。如果 alpha 为较小值，剪枝会更加激进；较大值时则不会轻易剪枝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9f3cd868-740a-4d74-9c0d-1f80a5881b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_tree(tree, validation_data, target_column='好瓜', alpha=0.01):\n",
    "    \"\"\"\n",
    "    后剪枝算法，递归删除不重要的分支，以减少过拟合。\n",
    "    \"\"\"\n",
    "    # 如果树是叶子节点，则直接返回\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    # 遍历树的所有子树\n",
    "    for feature, branches in tree.items():\n",
    "        for branch_condition, subtree in branches.items():\n",
    "            tree[feature][branch_condition] = prune_tree(subtree, validation_data, target_column, alpha)\n",
    "\n",
    "    # 在当前节点剪枝：计算剪枝前后的误差\n",
    "    error_before_pruning = calculate_tree_error(tree, validation_data, target_column)\n",
    "    error_after_pruning = calculate_tree_error_after_pruning(tree, validation_data, target_column)\n",
    "\n",
    "    # 如果剪枝后的误差不比剪枝前差，进行剪枝\n",
    "    if error_after_pruning <= error_before_pruning + alpha:\n",
    "        return get_leaf_value(tree)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def calculate_tree_error(tree, data, target_column='好瓜'):\n",
    "    \"\"\"\n",
    "    计算决策树在数据集上的误差\n",
    "    \"\"\"\n",
    "    predictions = data.apply(lambda row: predict_cart(tree, row), axis=1)\n",
    "    errors = predictions != data[target_column]\n",
    "    return errors.mean()  # 计算错误率\n",
    "\n",
    "\n",
    "def calculate_tree_error_after_pruning(tree, data, target_column='好瓜'):\n",
    "    \"\"\"\n",
    "    计算剪枝后树的误差（将所有分支替换为叶节点）\n",
    "    \"\"\"\n",
    "    pruned_tree = get_leaf_value(tree)\n",
    "    predictions = data.apply(lambda row: predict_cart(pruned_tree, row), axis=1)\n",
    "    errors = predictions != data[target_column]\n",
    "    return errors.mean()\n",
    "\n",
    "\n",
    "def get_leaf_value(tree):\n",
    "    \"\"\"\n",
    "    获取树的叶节点值（即，所有分支的预测结果）\n",
    "    \"\"\"\n",
    "    # 如果是叶子节点，直接返回\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    for feature, branches in tree.items():\n",
    "        for branch_condition, subtree in branches.items():\n",
    "            return get_leaf_value(subtree)\n",
    "\n",
    "\n",
    "# 构建 CART 决策树\n",
    "features_cart = [col for col in train2_dataset.columns if col not in ['好瓜', '编号']]\n",
    "\n",
    "# 构建初始 CART 决策树\n",
    "my_CART_tree = build_CART_tree(train2_dataset, features_cart)\n",
    "\n",
    "# 使用验证集进行剪枝\n",
    "validation_data = train2_dataset  \n",
    "pruned_tree = prune_tree(my_CART_tree, validation_data, target_column='好瓜', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ca4ade5e-0f2b-4f89-b13d-f1fc441b685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART决策树方法的准确度: 0.8\n"
     ]
    }
   ],
   "source": [
    "def calculate_cart_accuracy_and_predict(tree, test_data, target_column='好瓜'):\n",
    "    correct_predictions = 0\n",
    "    no_branch_predictions = 0  # 记录预测为 None 的样本数\n",
    "\n",
    "    # 遍历测试数据的每一行，进行预测\n",
    "    for _, row in test_data.iterrows():\n",
    "        # 预测一个样本\n",
    "        def predict_cart(tree, sample):\n",
    "            # 如果当前节点是叶节点，直接返回预测值\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree\n",
    "\n",
    "            # 遍历当前节点的特征及其子树\n",
    "            for feature, branches in tree.items():\n",
    "                if feature in sample:\n",
    "                    feature_value = sample[feature]\n",
    "\n",
    "                    # 如果是连续特征的分割，处理 '<=' 和 '>'\n",
    "                    if isinstance(branches, dict):\n",
    "                        for branch_condition in branches.keys():\n",
    "                            # 处理连续特征的分支\n",
    "                            if branch_condition.startswith('<='): \n",
    "                                threshold = float(branch_condition[3:])  # 阈值提取\n",
    "                                if feature_value <= threshold:\n",
    "                                    return predict_cart(branches[branch_condition], sample)\n",
    "                            elif branch_condition.startswith('>'):\n",
    "                                threshold = float(branch_condition[3:])  \n",
    "                                if feature_value > threshold:\n",
    "                                    return predict_cart(branches[branch_condition], sample)\n",
    "\n",
    "                    # 如果是离散特征的分支，直接递归访问子树\n",
    "                    if feature_value in branches:\n",
    "                        return predict_cart(branches[feature_value], sample)\n",
    "\n",
    "            # 如果没有找到合适的分支，返回 None（无法预测）\n",
    "            return None\n",
    "\n",
    "        # 进行预测\n",
    "        predicted_label = predict_cart(tree, row)\n",
    "        actual_label = row[target_column]\n",
    "\n",
    "        # 统计预测正确的样本\n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions += 1\n",
    "        elif predicted_label is None:\n",
    "            no_branch_predictions += 1  # 记录没有找到合适分支的情况\n",
    "\n",
    "    # 计算并返回准确率\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "# 计算并打印准确率\n",
    "cart_accuracy = calculate_cart_accuracy_and_predict(pruned_tree, test2_dataset)\n",
    "print(f\"CART决策树方法的准确度: {cart_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd2dfa-58fd-4f81-9dcb-8714f52a8de0",
   "metadata": {},
   "source": [
    "#### 发现剪枝后的精度仍为0.8，后剪枝没有使得决策树性能更佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57782b21-7872-41c9-8c43-5c806b39a374",
   "metadata": {},
   "source": [
    "### **分析精度下降或没有提高的原因**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09a03a-2456-4346-8a54-9f498b3cae40",
   "metadata": {},
   "source": [
    "**1. 剪枝引入的过度简化**\n",
    "\n",
    "剪枝的目标是通过减少树的复杂性来避免过拟合。然而，在某些情况下，过度剪枝会导致决策树缺乏足够的表达能力，从而降低模型的准确性。\n",
    "\n",
    "（1）过度剪枝：如果剪枝过早或过度，可能导致模型无法捕捉到训练数据中的复杂模式。尤其是在数据集较复杂或者特征之间的关联性较强的情况下，过度简化可能会减少模型的有效性，从而导致精度下降。\n",
    "\n",
    "（2）剪枝条件不合理：预剪枝和后剪枝通常都有一些条件，例如节点样本数低于某个阈值、信息增益降低等。如果这些条件设置得不合理，可能会导致模型在树的早期或深度不足的情况下进行剪枝，导致分类性能降低。\n",
    "\n",
    "**2. 后剪枝操作影响**\n",
    "\n",
    "\n",
    "（1）后剪枝是通过完全生成树后再进行的操作。尽管这种方法可以通过避免过早停止来防止欠拟合，但它也可能因为过度修剪去除了一些重要的分支，从而导致测试数据集的精度下降。\n",
    "\n",
    "（2）剪枝步骤：后剪枝通常是自底向上的操作，会从叶子节点逐步回溯，检查每个内部节点是否应该剪去。在进行后剪枝时，如果剪去的分支实际在测试集上具有较高的分类精度，精度可能会显著下降。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
